{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ipython\n"
      ],
      "metadata": {
        "id": "RNEjjzNYlkvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgG1_4Nba6Uq"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6OGgKGAwaTA"
      },
      "outputs": [],
      "source": [
        "!pip install plotnine xgboost Prophet optuna shap xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9U2qI1ka6KW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "np.set_printoptions(suppress=True)\n",
        "#suppress exponential notation, define an appropriate float formatter\n",
        "#specify stdout line width and let pretty print do the work\n",
        "np.set_printoptions(suppress=True, formatter={'float_kind':'{:16.3f}'.format}, linewidth=130)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.size'] = 18\n",
        "import seaborn as sns\n",
        "\n",
        "from plotnine import *\n",
        "\n",
        "\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'\n",
        "\n",
        "\n",
        "from IPython.core.debugger import set_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hv1sxKCa-aC"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R_qvKPZbjxF"
      },
      "outputs": [],
      "source": [
        "# !git config --global user.name \"Praveen Kumar Anwla\"\n",
        "# !git config --global user.email \"praveenkumar.kumar76@gmail.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kENMGUSoa3l0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Praveen76/MMM-using-Non-linear-models-and-SHAP-Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQa9EvZPbScI"
      },
      "outputs": [],
      "source": [
        "%cd MMM-using-Non-linear-models-and-SHAP-Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8TLWNA1cKH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eM5KwEJbJZW"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "shap.initjs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSimoV7kbJdf"
      },
      "outputs": [],
      "source": [
        "from prophet import Prophet\n",
        "\n",
        "from functools import partial\n",
        "import optuna as opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47agJ8-gbJgs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjj1GOKdbJjq"
      },
      "outputs": [],
      "source": [
        "START_ANALYSIS_INDEX = 52\n",
        "END_ANALYSIS_INDEX = 144"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAE8gkikbJmb"
      },
      "outputs": [],
      "source": [
        "data_org = pd.read_csv(\"./MMM_data.csv\", parse_dates = [\"DATE\"])\n",
        "data_org.columns = [c.lower() if c in [\"DATE\"] else c for c in data_org.columns]\n",
        "data_org\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E3gaM18dPmc"
      },
      "outputs": [],
      "source": [
        "# Import holidays dataset from prophet\n",
        "holidays = pd.read_csv(\"./prophet_holidays_daily.csv\", parse_dates = [\"ds\"])\n",
        "\n",
        "holidays[\"begin_week\"] = holidays[\"ds\"].dt.to_period('W-SUN').dt.start_time\n",
        "\n",
        "# #combine same week holidays into one holiday\n",
        "holidays_weekly = holidays.groupby([\"begin_week\", \"country\", \"year\"], as_index = False).agg({'holiday':'#'.join, 'country': 'first', 'year': 'first'}).rename(columns = {'begin_week': 'ds'})\n",
        "holidays_weekly_us = holidays_weekly.query(\"(country == 'US')\").copy()\n",
        "holidays_weekly_us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQcdOP9qoV2o"
      },
      "outputs": [],
      "source": [
        "data_org.events.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnMGXvMqhIE3"
      },
      "outputs": [],
      "source": [
        "df = data_org.rename(columns = {'revenue': 'y', 'date': 'ds'})\n",
        "\n",
        "#add categorical into prophet\n",
        "df = pd.concat([df, pd.get_dummies(df[\"events\"], drop_first = True, prefix = \"events\")], axis = 1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfUCAEU6hIJ9"
      },
      "outputs": [],
      "source": [
        "# Model should include yearly seasonality in the forecast with holiday effects being included in the model, and the holidays are provided in the holidays_weekly_us dataset.\n",
        "prophet = Prophet(yearly_seasonality=True, holidays=holidays_weekly_us)\n",
        "\n",
        "#A regressor named \"events_event2\" is added to the model. This regressor contains additional information or events that might impact the time series.\n",
        "prophet.add_regressor(name = \"events_event2\")\n",
        "\n",
        "#A regressor named \"events_na\" is added to the model. This regressor contains additional information or events that might impact the time series.\n",
        "prophet.add_regressor(name = \"events_na\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld4aLHYfpYOK"
      },
      "outputs": [],
      "source": [
        "# Train the prophet model using date column called ds, target variable called 'y', and additional features/regressors like events_event2 & events_na\n",
        "prophet.fit(df[[\"ds\", \"y\", \"events_event2\", \"events_na\"]])\n",
        "prophet_prediction = prophet.predict(df[[\"ds\", \"y\", \"events_event2\", \"events_na\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CjCnfmupzpA"
      },
      "outputs": [],
      "source": [
        "plot = prophet.plot_components(prophet_prediction, figsize = (20, 10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SDkK-UzrLmG"
      },
      "outputs": [],
      "source": [
        "# Let's understand above 4 separate plots showing different aspects of the data:\n",
        "\n",
        "# - The 1st plot shows the trend component, indicating a general increase in values over time with some seasonality.\n",
        "# - The 2nd plot shows holidays from 2015 to 2019\n",
        "# - The 3rd plot illustrates yearly seasonality, capturing patterns that repeat annually.\n",
        "# - The 4th plot represents an additive regression component, highlighting specific points in time where significant changes occurred.\n",
        "\n",
        "# The blue line in each plot represents the model prediction, and the shaded area represents the uncertainty interval.\n",
        "# The x-axis is the date (ds), ranging from October 2015 to October 2019.\n",
        "# The y-axis varies depending on the plot: it is labeled as \"trend\", \"holidays\", \"yearly\", or \"multiplicative/additive\" respectively.\n",
        "\n",
        "# The above charts can help you understand how the model decomposes the time series data into different components, and how confident it is about its predictions.\n",
        "#  You can also use the chart to compare the model predictions with the actual data, and evaluate the model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z_bYzdPtV5c"
      },
      "outputs": [],
      "source": [
        "prophet_prediction.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Af1yVjrs87s"
      },
      "outputs": [],
      "source": [
        "# Filter out columns whose names end with \"upper\" or \"lower,\" because these columns represent upper and lower bounds of prediction intervals.\n",
        "prophet_columns = [col for col in prophet_prediction.columns if (col.endswith(\"upper\") == False) & (col.endswith(\"lower\") == False)]\n",
        "\n",
        "# keep only the columns with names containing \"events_\", and sum the values across columns for these events_ columns.\n",
        "events_nums = prophet_prediction[prophet_columns].filter(like = \"events_\").sum(axis = 1)\n",
        "\n",
        "new_data = data_org.copy()\n",
        "new_data[\"trend\"] = prophet_prediction[\"trend\"]\n",
        "new_data[\"season\"] = prophet_prediction[\"yearly\"]\n",
        "new_data[\"holiday\"] = prophet_prediction[\"holidays\"]\n",
        "new_data[\"events\"] = (events_nums - np.min(events_nums)).values  # Scale events_nums values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px_cze8ob2xC"
      },
      "outputs": [],
      "source": [
        "new_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUG8t_eKv4La"
      },
      "outputs": [],
      "source": [
        "data_org.iloc[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-qeypI3wFBP"
      },
      "outputs": [],
      "source": [
        "data = new_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqYpEmkxwZBv"
      },
      "outputs": [],
      "source": [
        "target = \"revenue\"\n",
        "media_channels = [\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_S\", \"search_S\"]\n",
        "organic_channels = [\"newsletter\"]\n",
        "features = [\"trend\", \"season\", \"holiday\", \"competitor_sales_B\", \"events\"] + media_channels + organic_channels\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM87Jcikwh2z"
      },
      "outputs": [],
      "source": [
        "corr = data.loc[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX - 1, [target] + features].corr()\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(25,10))\n",
        "sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, annot = True, cmap = sns.diverging_palette(220, 20, as_cmap=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKex-wcYwvRU"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils import check_array\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "\n",
        "# Apply a geometric decay transformation to this time series data.\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import check_array, check_is_fitted\n",
        "import numpy as np\n",
        "\n",
        "# Write a custom transformer for applying geometric decay transformation (Adstock) to the time series data.\n",
        "class AdstockGeometric(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    alpha : float, optional, default: 0.5\n",
        "        The decay factor controlling how fast the weights decrease exponentially.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(X, y=None):\n",
        "        Fit the adstock transformer to the input time series data.\n",
        "\n",
        "    transform(X):\n",
        "        Apply adstock geometric decay transformation to the input time series.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.5):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the adstock transformer to the input time series data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like or pd.DataFrame\n",
        "            Input time series data.\n",
        "        y : None\n",
        "            Ignored. Not used in the transformation.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        self : object\n",
        "            Returns an instance of the fitted transformer.\n",
        "\n",
        "        \"\"\"\n",
        "        X = check_array(X)\n",
        "        self._check_n_features(X, reset=True)\n",
        "        self.fitted_ = True  # Corrected attribute name\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Apply adstock geometric decay transformation to the input time series.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like or pd.DataFrame\n",
        "            Input time series data.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        x_decayed : np.ndarray\n",
        "            Transformed time series with adstock effect applied.\n",
        "\n",
        "        \"\"\"\n",
        "        check_is_fitted(self)\n",
        "        X = check_array(X)\n",
        "        self._check_n_features(X, reset=False)\n",
        "        x_decayed = np.zeros_like(X)\n",
        "        x_decayed[0] = X[0]\n",
        "\n",
        "        for xi in range(1, len(x_decayed)):\n",
        "            x_decayed[xi] = X[xi] + self.alpha * x_decayed[xi - 1]\n",
        "\n",
        "        return x_decayed\n",
        "\n",
        "    def _check_n_features(self, X, reset=False):\n",
        "        \"\"\"\n",
        "        Helper method to check the number of features in the input data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like or pd.DataFrame\n",
        "            Input time series data.\n",
        "        reset : bool, optional, default: False\n",
        "            Ignored. Not used in this implementation.\n",
        "\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "# Define evaluation metrics\n",
        "def nrmse(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2)) / (np.max(y_true) - np.min(y_true))\n",
        "\n",
        "#https://github.com/facebookexperimental/Robyn\n",
        "def rssd(effect_share, spend_share):\n",
        "    \"\"\"RSSD decomposition\n",
        "\n",
        "    Decomposition distance (root-sum-square distance, a major innovation of Robyn)\n",
        "    eliminates the majority of \"bad models\"\n",
        "    (larger prediction error and/or unrealistic media effect like the smallest channel getting the most effect\n",
        "\n",
        "    Args:\n",
        "        effect_share ([type]): percentage of effect share\n",
        "        spend_share ([type]): percentage of spend share\n",
        "\n",
        "    Returns:\n",
        "        [type]: [description]\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.sum((effect_share - spend_share) ** 2))\n",
        "\n",
        "\n",
        "def plot_spend_vs_effect_share(decomp_spend: pd.DataFrame, figure_size = (15, 10)):\n",
        "    \"\"\"Spend vs Effect Share plot\n",
        "\n",
        "    Args:\n",
        "        decomp_spend (pd.DataFrame): Data with media decompositions. The following columns should be present: media, spend_share, effect_share per media variable\n",
        "        figure_size (tuple, optional): Figure size. Defaults to (15, 10).\n",
        "\n",
        "    Example:\n",
        "        decomp_spend:\n",
        "        media         spend_share effect_share\n",
        "        tv_S           0.31        0.44\n",
        "        ooh_S          0.23        0.34\n",
        "\n",
        "    Returns:\n",
        "        [plotnine]: plotnine plot\n",
        "    \"\"\"\n",
        "\n",
        "    plot_spend_effect_share = decomp_spend.melt(id_vars = [\"media\"], value_vars = [\"spend_share\", \"effect_share\"])\n",
        "\n",
        "    plt = ggplot(plot_spend_effect_share, aes(\"media\", \"value\", fill = \"variable\")) \\\n",
        "    + geom_bar(stat = \"identity\", position = \"dodge\") \\\n",
        "    + geom_text(aes(label = \"value * 100\", group = \"variable\"), color = \"darkblue\", position=position_dodge(width = 0.5), format_string = \"{:.2f}%\") \\\n",
        "    + coord_flip() \\\n",
        "    + ggtitle(\"Share of Spend VS Share of Effect\") + ylab(\"\") + xlab(\"\") \\\n",
        "    + theme(figure_size = figure_size,\n",
        "                    legend_direction='vertical',\n",
        "                    legend_title=element_blank(),\n",
        "                    legend_key_size=20,\n",
        "                    legend_entry_spacing_y=5)\n",
        "    return plt\n",
        "\n",
        "\n",
        "def calculate_spend_effect_share(df_shap_values: pd.DataFrame, media_channels, df_original: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        df_shap_values: data frame of shap values\n",
        "        media_channels: list of media channel names\n",
        "        df_original: non transformed original data\n",
        "    Returns:\n",
        "        [pd.DataFrame]: data frame with spend effect shares\n",
        "    \"\"\"\n",
        "    responses = pd.DataFrame(df_shap_values[media_channels].abs().sum(axis = 0), columns = [\"effect_share\"])\n",
        "    response_percentages = responses / responses.sum()\n",
        "    response_percentages\n",
        "\n",
        "    spends_percentages = pd.DataFrame(df_original[media_channels].sum(axis = 0) / df_original[media_channels].sum(axis = 0).sum(), columns = [\"spend_share\"])\n",
        "    spends_percentages\n",
        "\n",
        "    spend_effect_share = pd.merge(response_percentages, spends_percentages, left_index = True, right_index = True)\n",
        "    spend_effect_share = spend_effect_share.reset_index().rename(columns = {\"index\": \"media\"})\n",
        "\n",
        "    return spend_effect_share\n",
        "\n",
        "\n",
        "#https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d\n",
        "def shap_feature_importance(shap_values, data, figsize = (20, 10)):\n",
        "\n",
        "    feature_list = data.columns\n",
        "\n",
        "    if isinstance(shap_values, pd.DataFrame) == False:\n",
        "        shap_v = pd.DataFrame(shap_values)\n",
        "        shap_v.columns = feature_list\n",
        "    else:\n",
        "        shap_v = shap_values\n",
        "\n",
        "\n",
        "    df_v = data.copy().reset_index().drop('index',axis=1)\n",
        "\n",
        "    # Determine the correlation in order to plot with different colors\n",
        "    corr_list = list()\n",
        "    for i in feature_list:\n",
        "        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n",
        "        corr_list.append(b)\n",
        "    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)\n",
        "    # Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient\n",
        "    corr_df.columns  = ['Variable','Corr']\n",
        "    corr_df['Sign'] = np.where(corr_df['Corr']>0,'red','blue')\n",
        "\n",
        "    # Plot it\n",
        "    shap_abs = np.abs(shap_v)\n",
        "    k=pd.DataFrame(shap_abs.mean()).reset_index()\n",
        "    k.columns = ['Variable','SHAP_abs']\n",
        "    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
        "    k2 = k2.sort_values(by='SHAP_abs',ascending = True)\n",
        "    colorlist = k2['Sign']\n",
        "    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=figsize,legend=False)\n",
        "    ax.set_xlabel(\"SHAP Value (Red = Positive Impact)\")\n",
        "\n",
        "def model_refit(data,\n",
        "                target,\n",
        "                features,\n",
        "                media_channels,\n",
        "                organic_channels,\n",
        "                model_params,\n",
        "                adstock_params,\n",
        "                start_index,\n",
        "                end_index):\n",
        "    data_refit = data.copy()\n",
        "\n",
        "    best_params = model_params\n",
        "\n",
        "    adstock_alphas = adstock_params\n",
        "\n",
        "    #apply adstock transformation\n",
        "    for feature in media_channels + organic_channels:\n",
        "        adstock_alpha = adstock_alphas[feature]\n",
        "        print(f\"applying geometric adstock transformation on {feature} with alpha {adstock_alpha}\")\n",
        "\n",
        "        #adstock transformation\n",
        "        x_feature = data_refit[feature].values.reshape(-1, 1)\n",
        "        temp_adstock = AdstockGeometric(alpha = adstock_alpha).fit_transform(x_feature)\n",
        "        data_refit[feature] = temp_adstock\n",
        "\n",
        "    #build the final model on the data until the end analysis index\n",
        "    x_input = data_refit.loc[0:end_index-1, features]\n",
        "    y_true_all = data[target].values[0:end_index]\n",
        "\n",
        "    #build random forest using the best parameters\n",
        "    xgboost = XGBRegressor(random_state=0, **best_params)\n",
        "    xgboost.fit(x_input, y_true_all)\n",
        "\n",
        "\n",
        "    #concentrate on the analysis interval\n",
        "    y_true_interval = y_true_all[start_index:end_index]\n",
        "    x_input_interval_transformed = x_input.iloc[start_index:end_index]\n",
        "\n",
        "    #revenue prediction for the analysis interval\n",
        "    print(f\"predicting {len(x_input_interval_transformed)}\")\n",
        "    prediction = xgboost.predict(x_input_interval_transformed)\n",
        "\n",
        "    #transformed data set for the analysis interval\n",
        "    x_input_interval_nontransformed = data.iloc[start_index:end_index]\n",
        "\n",
        "\n",
        "    #shap explainer\n",
        "    explainer = shap.TreeExplainer(xgboost)\n",
        "\n",
        "    # get SHAP values for the data set for the analysis interval from explainer model\n",
        "    shap_values_train = explainer.shap_values(x_input_interval_transformed)\n",
        "\n",
        "    # create a dataframe of the shap values for the training set and the test set\n",
        "    df_shap_values = pd.DataFrame(shap_values_train, columns=features)\n",
        "\n",
        "    return {\n",
        "            'df_shap_values': df_shap_values,\n",
        "            'x_input_interval_nontransformed': x_input_interval_nontransformed,\n",
        "            'x_input_interval_transformed' : x_input_interval_transformed,\n",
        "            'prediction_interval': prediction,\n",
        "            'y_true_interval': y_true_interval\n",
        "           }\n",
        "\n",
        "def plot_shap_vs_spend(df_shap_values, x_input_interval_nontransformed, x_input_interval_transformed, features, media_channels, figsize=(25, 10)):\n",
        "    for channel in media_channels:\n",
        "\n",
        "        #index = features.index(channel)\n",
        "\n",
        "        mean_spend = x_input_interval_nontransformed.loc[x_input_interval_nontransformed[channel] > 0, channel].mean()\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        sns.regplot(x = x_input_interval_transformed[channel], y = df_shap_values[channel], label = channel,\n",
        "                    scatter_kws={'alpha': 0.65}, line_kws={'color': 'C2', 'linewidth': 6},\n",
        "                    lowess=True, ax=ax).set(title=f'{channel}: Spend vs Shapley')\n",
        "        ax.axhline(0, linestyle = \"--\", color = \"black\", alpha = 0.5)\n",
        "        ax.axvline(mean_spend, linestyle = \"--\", color = \"red\", alpha = 0.5, label=f\"Average Spend: {int(mean_spend)}\")\n",
        "        ax.set_xlabel(f\"{channel} spend\")\n",
        "        ax.set_ylabel(f'SHAP Value for {channel}')\n",
        "        plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s_KROCSeAyg"
      },
      "outputs": [],
      "source": [
        "media_channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBqNzyZT0QR5"
      },
      "outputs": [],
      "source": [
        "# Modeling with XgBoost\n",
        "def optuna_trial(trial,\n",
        "                 data:pd.DataFrame,\n",
        "                 target,\n",
        "                 features,\n",
        "                 adstock_features,\n",
        "                 adstock_features_params,\n",
        "                 media_features,\n",
        "                 tscv,\n",
        "                 is_multiobjective = False):\n",
        "\n",
        "    data_temp = data.copy()\n",
        "    adstock_alphas = {}\n",
        "\n",
        "    for feature in adstock_features:\n",
        "        adstock_param = f\"{feature}_adstock\"\n",
        "        min_, max_ = adstock_features_params[adstock_param]\n",
        "        adstock_alpha = trial.suggest_float(f\"adstock_alpha_{feature}\", min_, max_)\n",
        "        adstock_alphas[feature] = adstock_alpha\n",
        "\n",
        "        ############ adstock transformation ############\n",
        "        x_feature = data[feature].values.reshape(-1, 1)\n",
        "\n",
        "        # Create an instance of AdstockGeometric\n",
        "        adstock_transformer = AdstockGeometric(alpha=adstock_alpha)\n",
        "\n",
        "        # Fit the transformer on the training data\n",
        "        adstock_transformer.fit(x_feature)\n",
        "\n",
        "        # Transform the data using the fitted transformer\n",
        "        temp_adstock = adstock_transformer.transform(x_feature)\n",
        "\n",
        "        data_temp[feature] = temp_adstock #Replace existing media channels values with transformed adstock values\n",
        "        ################################################\n",
        "\n",
        "    #XgBoost parameters\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 5, 100),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 7),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 1),\n",
        "    }\n",
        "    scores = []\n",
        "\n",
        "    rssds = []\n",
        "    for train_index, test_index in tscv.split(data_temp):\n",
        "        x_train = data_temp.iloc[train_index][features]\n",
        "        y_train =  data_temp[target].values[train_index]\n",
        "\n",
        "        x_test = data_temp.iloc[test_index][features]\n",
        "        y_test = data_temp[target].values[test_index]\n",
        "\n",
        "        #apply XgBoost\n",
        "\n",
        "        xgboost = XGBRegressor(random_state=0, **params)\n",
        "        xgboost.fit(x_train, y_train)\n",
        "        prediction = xgboost.predict(x_test)\n",
        "\n",
        "        # rmse = mean_squared_error(y_true = y_test, y_pred = prediction, squared = False)\n",
        "\n",
        "\n",
        "        mse = mean_squared_error(y_test, prediction)   # no 'squared' kw\n",
        "        rmse = np.sqrt(mse)\n",
        "        scores.append(rmse)\n",
        "\n",
        "        if is_multiobjective:\n",
        "\n",
        "            #set_trace()\n",
        "            #calculate spend effect share -> rssd\n",
        "            # create explainer model by passing trained model to shap\n",
        "            explainer = shap.TreeExplainer(xgboost)\n",
        "\n",
        "            # get Shap values\n",
        "            shap_values_train = explainer.shap_values(x_train)\n",
        "\n",
        "            df_shap_values = pd.DataFrame(shap_values_train, columns=features)\n",
        "\n",
        "            spend_effect_share = calculate_spend_effect_share(df_shap_values = df_shap_values, media_channels = media_features, df_original = data.iloc[train_index])\n",
        "\n",
        "            decomp_rssd = rssd(effect_share = spend_effect_share.effect_share.values, spend_share = spend_effect_share.spend_share.values)\n",
        "            rssds.append(decomp_rssd)\n",
        "\n",
        "    trial.set_user_attr(\"scores\", scores)\n",
        "\n",
        "    trial.set_user_attr(\"params\", params)\n",
        "    trial.set_user_attr(\"adstock_alphas\", adstock_alphas)\n",
        "\n",
        "    if is_multiobjective == False:\n",
        "        return np.mean(scores)\n",
        "\n",
        "\n",
        "    trial.set_user_attr(\"rssds\", rssds)\n",
        "\n",
        "    #multiobjective\n",
        "    return np.mean(scores), np.mean(rssds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftHPXhGe1sQ-"
      },
      "outputs": [],
      "source": [
        "def optuna_optimize(trials,\n",
        "                    data: pd.DataFrame,\n",
        "                    target,\n",
        "                    features,\n",
        "                    adstock_features,\n",
        "                    adstock_features_params,\n",
        "                    media_features,\n",
        "                    tscv,\n",
        "                    is_multiobjective,\n",
        "                    seed = 42):\n",
        "    print(f\"data size: {len(data)}\")\n",
        "    print(f\"media features: {media_features}\")\n",
        "    print(f\"adstock features: {adstock_features}\")\n",
        "    print(f\"features: {features}\")\n",
        "    print(f\"is_multiobjective: {is_multiobjective}\")\n",
        "    opt.logging.set_verbosity(opt.logging.WARNING)\n",
        "\n",
        "    if is_multiobjective == False:\n",
        "        study_mmm = opt.create_study(direction='minimize', sampler = opt.samplers.TPESampler(seed=seed)) #Tree-structured Parzen Estimator (TPESampler)\n",
        "    else:\n",
        "        study_mmm = opt.create_study(directions=[\"minimize\", \"minimize\"], sampler=opt.samplers.NSGAIISampler(seed=seed)) # NSGA-II (Non-dominated Sorting Genetic Algorithm II).\n",
        "\n",
        "    #It fixes certain parameters (those passed to optuna_trial that don't change during optimization).\n",
        "    optimization_function = partial(optuna_trial,\n",
        "                                    data = data,\n",
        "                                    target = target,\n",
        "                                    features = features,\n",
        "                                    adstock_features = adstock_features,\n",
        "                                    adstock_features_params = adstock_features_params,\n",
        "                                    media_features = media_features,\n",
        "                                    tscv = tscv,\n",
        "                                    is_multiobjective = is_multiobjective)\n",
        "\n",
        "    #The optimization function (optuna_trial) is executed iteratively with different sets of hyperparameters, and the study seeks to minimize the objective function.\n",
        "    study_mmm.optimize(optimization_function, n_trials = trials, show_progress_bar = True)\n",
        "\n",
        "    # returns the Optuna study (study_mmm) after optimization.\n",
        "    return study_mmm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoYYZw3MdOcS"
      },
      "outputs": [],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTlQPY4ldBDs"
      },
      "outputs": [],
      "source": [
        "media_channels + organic_channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC4fVV4QdNPo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oiLkzVV13l1"
      },
      "outputs": [],
      "source": [
        "tscv = TimeSeriesSplit(n_splits=3, test_size = 20)\n",
        "\n",
        "adstock_features_params = {}\n",
        "adstock_features_params[\"tv_S_adstock\"] = (0.3, 0.8)\n",
        "adstock_features_params[\"ooh_S_adstock\"] = (0.1, 0.4)\n",
        "adstock_features_params[\"print_S_adstock\"] = (0.1, 0.4)\n",
        "adstock_features_params[\"facebook_S_adstock\"] = (0.0, 0.4)\n",
        "adstock_features_params[\"search_S_adstock\"] = (0.0, 0.3)\n",
        "adstock_features_params[\"newsletter_adstock\"] = (0.1, 0.4)\n",
        "\n",
        "# OPTUNA_TRIALS = 2000\n",
        "OPTUNA_TRIALS = 2\n",
        "experiment = optuna_optimize(trials = OPTUNA_TRIALS,\n",
        "                             data = data,\n",
        "                             target = target,\n",
        "                             features = features,\n",
        "                             adstock_features = media_channels + organic_channels,\n",
        "                             adstock_features_params = adstock_features_params,\n",
        "                             media_features=media_channels,\n",
        "                             tscv = tscv,\n",
        "                             is_multiobjective=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwZ3AjG716UW"
      },
      "outputs": [],
      "source": [
        "experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZkZuTzxBjvV"
      },
      "outputs": [],
      "source": [
        "experiment.best_trial.user_attrs[\"scores\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKVqqlrXBkAd"
      },
      "outputs": [],
      "source": [
        "experiment.best_trial.user_attrs[\"params\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oHix3HxBliu"
      },
      "outputs": [],
      "source": [
        "experiment.best_trial.user_attrs[\"adstock_alphas\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhRnHxYGBmou"
      },
      "outputs": [],
      "source": [
        "experiment.best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzRxs-QYBnre"
      },
      "outputs": [],
      "source": [
        "experiment.best_trial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqQsdPZmBpEM"
      },
      "outputs": [],
      "source": [
        "np.mean(experiment.best_trial.user_attrs[\"scores\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWY7krVfBsAl"
      },
      "outputs": [],
      "source": [
        "experiment.best_trial.user_attrs[\"params\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBoLZ9zZBtCe"
      },
      "outputs": [],
      "source": [
        "experiment.best_trial.user_attrs[\"adstock_alphas\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-HXzTNiBuDD"
      },
      "outputs": [],
      "source": [
        "# Model Refit\n",
        "best_params = experiment.best_trial.user_attrs[\"params\"]\n",
        "adstock_params = experiment.best_trial.user_attrs[\"adstock_alphas\"]\n",
        "result = model_refit(data = data,\n",
        "                     target = target,\n",
        "                     features = features,\n",
        "                     media_channels = media_channels,\n",
        "                     organic_channels = organic_channels,\n",
        "                     model_params = best_params,\n",
        "                     adstock_params = adstock_params,\n",
        "                     start_index = START_ANALYSIS_INDEX,\n",
        "                     end_index = END_ANALYSIS_INDEX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd2CcafCB8Ar"
      },
      "outputs": [],
      "source": [
        "type(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKsbhlSKCen-"
      },
      "outputs": [],
      "source": [
        "result.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upKpicECCl1V"
      },
      "outputs": [],
      "source": [
        "\n",
        "!mkdir results\n",
        "for key, value in result.items():\n",
        "  if type(value) == pd.DataFrame:\n",
        "\n",
        "    result[key].to_csv(f\"./results/{key}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd19_TMyDg2x"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'prediction_interval': result['prediction_interval'] }).to_csv(f\"./results/prediction_interval.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i6EGf4HDSgT"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'y_true_interval': result['y_true_interval'] }).to_csv(f\"./results/y_true_interval.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1DenCN0BvnI"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "# rmse_metric = mean_squared_error(y_true = result[\"y_true_interval\"], y_pred = result[\"prediction_interval\"], squared=False)\n",
        "\n",
        "mse = mean_squared_error(y_true = result[\"y_true_interval\"], y_pred = result[\"prediction_interval\"])\n",
        "rmse_metric = np.sqrt(mse)\n",
        "\n",
        "\n",
        "mape_metric = mean_absolute_percentage_error(y_true = result[\"y_true_interval\"], y_pred = result[\"prediction_interval\"])\n",
        "nrmse_metric = nrmse(result[\"y_true_interval\"], result[\"prediction_interval\"])\n",
        "r2_metric = r2_score(y_true = result[\"y_true_interval\"], y_pred = result[\"prediction_interval\"])\n",
        "\n",
        "print(f'RMSE: {rmse_metric}')\n",
        "print(f'MAPE: {mape_metric}')\n",
        "print(f'NRMSE: {nrmse_metric}')\n",
        "print(f'R2: {r2_metric}')\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (25, 8))\n",
        "_ = ax.plot(result[\"prediction_interval\"], color = \"blue\", label = \"predicted\")\n",
        "_ = ax.plot(result[\"y_true_interval\"], 'ro', label = \"true\")\n",
        "_ = plt.title(f\"RMSE: {np.round(rmse_metric)}, NRMSE: {np.round(nrmse_metric, 3)}, MAPE: {np.round(mape_metric, 3)}, R2: {np.round(r2_metric,3)}\")\n",
        "_ = ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaoyB6TLDyWg"
      },
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "shap_feature_importance(result[\"df_shap_values\"], result[\"x_input_interval_transformed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDHk4QvQD4Np"
      },
      "outputs": [],
      "source": [
        "spend_effect_share = calculate_spend_effect_share(df_shap_values = result[\"df_shap_values\"], media_channels = media_channels, df_original = result[\"x_input_interval_nontransformed\"])\n",
        "\n",
        "decomp_rssd = rssd(effect_share = spend_effect_share.effect_share.values, spend_share = spend_effect_share.spend_share.values)\n",
        "print(f\"DECOMP.RSSD: {decomp_rssd}\")\n",
        "print(plot_spend_vs_effect_share(spend_effect_share, figure_size = (15, 7)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwY_Yhb6zUdf"
      },
      "outputs": [],
      "source": [
        "!pip install -q statsmodels\n",
        "import statsmodels.api as sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP_1VPMGD7Of"
      },
      "outputs": [],
      "source": [
        "# plot_shap_vs_spend(result[\"df_shap_values\"], result[\"x_input_interval_nontransformed\"], result[\"x_input_interval_transformed\"], features, media_channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkxL8fNWD9vR"
      },
      "outputs": [],
      "source": [
        "# Multiobjective\n",
        "tscv = TimeSeriesSplit(n_splits=3, test_size = 10)\n",
        "\n",
        "# OPTUNA_MULTIOBJECTIVE_TRIALS = 1000\n",
        "OPTUNA_MULTIOBJECTIVE_TRIALS = 2\n",
        "\n",
        "adstock_features_params = {}\n",
        "adstock_features_params[\"tv_S_adstock\"] = (0.3, 0.8)\n",
        "adstock_features_params[\"ooh_S_adstock\"] = (0.1, 0.4)\n",
        "adstock_features_params[\"print_S_adstock\"] = (0.1, 0.4)\n",
        "adstock_features_params[\"facebook_S_adstock\"] = (0.0, 0.4)\n",
        "adstock_features_params[\"search_S_adstock\"] = (0.0, 0.3)\n",
        "adstock_features_params[\"newsletter_adstock\"] = (0.1, 0.4)\n",
        "\n",
        "experiment_multi = optuna_optimize(trials = OPTUNA_MULTIOBJECTIVE_TRIALS,\n",
        "                                   data = data,\n",
        "                                   target = target,\n",
        "                                   features = features,\n",
        "                                   adstock_features=media_channels + organic_channels,\n",
        "                                   adstock_features_params = adstock_features_params,\n",
        "                                   media_features=media_channels,\n",
        "                                   tscv = tscv,\n",
        "                                   is_multiobjective=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyU5IFzkEDy4"
      },
      "outputs": [],
      "source": [
        "fig = opt.visualization.plot_pareto_front(experiment_multi, target_names = [\"RMSE\", \"RSSD\"])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu7HOiLWEKDj"
      },
      "outputs": [],
      "source": [
        "#how many best models\n",
        "len(experiment_multi.best_trials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oLDMsZOMY3w"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "def final_model_refit(data,\n",
        "                target,\n",
        "                features,\n",
        "                media_channels,\n",
        "                organic_channels,\n",
        "                model_params,\n",
        "                adstock_params,\n",
        "                start_index,\n",
        "                end_index):\n",
        "    data_refit = data.copy()\n",
        "\n",
        "    best_params = model_params\n",
        "\n",
        "    adstock_alphas = adstock_params\n",
        "\n",
        "    #apply adstock transformation\n",
        "    for feature in media_channels + organic_channels:\n",
        "        adstock_alpha = adstock_alphas[feature]\n",
        "        print(f\"applying geometric adstock transformation on {feature} with alpha {adstock_alpha}\")\n",
        "\n",
        "        #adstock transformation\n",
        "        x_feature = data_refit[feature].values.reshape(-1, 1)\n",
        "        temp_adstock = AdstockGeometric(alpha = adstock_alpha).fit_transform(x_feature)\n",
        "        data_refit[feature] = temp_adstock\n",
        "\n",
        "    #build the final model on the data until the end analysis index\n",
        "    x_input = data_refit.loc[0:end_index-1, features]\n",
        "    y_true_all = data[target].values[0:end_index]\n",
        "\n",
        "    #build random forest using the best parameters\n",
        "    xgboost = XGBRegressor(random_state=0, **best_params)\n",
        "    xgboost.fit(x_input, y_true_all)\n",
        "\n",
        "\n",
        "    # Save the model to a file using pickle\n",
        "    model_filename = \"./final_xgboost_model.pkl\"\n",
        "    with open(model_filename, 'wb') as model_file:\n",
        "        pickle.dump(xgboost, model_file)\n",
        "\n",
        "    print(f\"Final model saved to {model_filename}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JV4pkcoENbv"
      },
      "outputs": [],
      "source": [
        "# Iterate through best models and rebuild\n",
        "multi_results = []\n",
        "for trial in experiment_multi.best_trials:\n",
        "    multi_result = model_refit(data = data,\n",
        "                     target = target,\n",
        "                     features = features,\n",
        "                     media_channels = media_channels,\n",
        "                     organic_channels = organic_channels,\n",
        "                     model_params = trial.user_attrs[\"params\"],\n",
        "                     adstock_params = trial.user_attrs[\"adstock_alphas\"],\n",
        "                     start_index = START_ANALYSIS_INDEX,\n",
        "                     end_index = END_ANALYSIS_INDEX)\n",
        "\n",
        "    print(f'RMSE: {np.sqrt(mean_squared_error(y_true = multi_result[\"y_true_interval\"], y_pred = multi_result[\"prediction_interval\"]))}')\n",
        "    print(f'MAPE: {mean_absolute_percentage_error(y_true = multi_result[\"y_true_interval\"], y_pred = multi_result[\"prediction_interval\"])}')\n",
        "    print(f'NRMSE: {nrmse(multi_result[\"y_true_interval\"], multi_result[\"prediction_interval\"])}')\n",
        "    print(f'R2: {r2_score(y_true = multi_result[\"y_true_interval\"], y_pred = multi_result[\"prediction_interval\"])}')\n",
        "    print(\"\")\n",
        "\n",
        "    spend_effect_share_multi = calculate_spend_effect_share(df_shap_values = multi_result[\"df_shap_values\"], media_channels = media_channels, df_original = multi_result[\"x_input_interval_nontransformed\"])\n",
        "    decomp_rssd_multi = rssd(effect_share = spend_effect_share_multi.effect_share.values, spend_share = spend_effect_share_multi.spend_share.values)\n",
        "    print(f\"DECOMP.RSSD: {decomp_rssd_multi}\")\n",
        "    print(plot_spend_vs_effect_share(spend_effect_share_multi, figure_size = (15, 7)))\n",
        "\n",
        "    multi_results.append(multi_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMI255S4OmcX"
      },
      "outputs": [],
      "source": [
        "final_model_refit(data = data,\n",
        "                  target = target,\n",
        "                  features = features,\n",
        "                  media_channels = media_channels,\n",
        "                  organic_channels = organic_channels,\n",
        "                  model_params = experiment_multi.best_trials[0].user_attrs[\"params\"],\n",
        "                  adstock_params = experiment_multi.best_trials[0].user_attrs[\"adstock_alphas\"],\n",
        "                  start_index = START_ANALYSIS_INDEX,\n",
        "                  end_index = END_ANALYSIS_INDEX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jpNwAp_Qjt8"
      },
      "outputs": [],
      "source": [
        "data.iloc[END_ANALYSIS_INDEX:END_ANALYSIS_INDEX+4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iQUYAMqQPRh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Specify the start and end indices for the test\n",
        "START_TEST_INDEX = END_ANALYSIS_INDEX\n",
        "END_TEST_INDEX = END_ANALYSIS_INDEX+4\n",
        "test_data = data\n",
        "# # Specify your target, features, media channels, and organic channels\n",
        "# target = \"revenue\"\n",
        "# media_channels = [\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_S\", \"search_S\"]\n",
        "# organic_channels = [\"newsletter\"]\n",
        "# features = [\"trend\", \"season\", \"holiday\", \"competitor_sales_B\", \"events\"] + media_channels + organic_channels\n",
        "\n",
        "# Load the final XGBoost model\n",
        "model_filename = \"./final_xgboost_model.pkl\"  # Replace with the actual filename\n",
        "with open(model_filename, 'rb') as model_file:\n",
        "    final_xgboost_model = pickle.load(model_file)\n",
        "\n",
        "# Apply adstock transformation to the test data\n",
        "adstock_params = {'tv_S': 0.5, 'ooh_S': 0.3, 'print_S': 0.2, 'facebook_S': 0.1, 'search_S': 0.0, 'newsletter': 0.2}\n",
        "for feature in media_channels + organic_channels:\n",
        "    adstock_alpha = adstock_params[feature]\n",
        "    x_feature = test_data[feature].values.reshape(-1, 1)\n",
        "    temp_adstock = AdstockGeometric(alpha=adstock_alpha).fit_transform(x_feature)\n",
        "    test_data[feature] = temp_adstock\n",
        "\n",
        "# Extract features and target for the test dataset\n",
        "x_test = test_data.loc[START_TEST_INDEX:END_TEST_INDEX-1, features]\n",
        "y_true_test = test_data[target].values[START_TEST_INDEX:END_TEST_INDEX]\n",
        "\n",
        "# Make predictions using the final model\n",
        "predictions_test = final_xgboost_model.predict(x_test)\n",
        "\n",
        "# Evaluate the performance metrics\n",
        "rmse_test = np.sqrt(mean_squared_error(y_true=y_true_test, y_pred=predictions_test))\n",
        "mae_test = mean_absolute_error(y_true=y_true_test, y_pred=predictions_test)\n",
        "r2_test = r2_score(y_true=y_true_test, y_pred=predictions_test)\n",
        "\n",
        "print(f'Test RMSE: {rmse_test}')\n",
        "print(f'Test MAE: {mae_test}')\n",
        "print(f'Test R2 Score: {r2_test}')\n",
        "\n",
        "# Optionally, you can visualize the predictions and true values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(y_true_test, label='True Values', marker='o')\n",
        "plt.plot(predictions_test, label='Predictions', marker='o')\n",
        "plt.title('Test Predictions vs True Values')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Revenue')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqSmaZs6GaH1"
      },
      "outputs": [],
      "source": [
        "\n",
        "!mkdir multi_result\n",
        "for key, value in multi_result.items():\n",
        "  if type(value) == pd.DataFrame:\n",
        "\n",
        "    result[key].to_csv(f\"./multi_result/{key}.csv\")\n",
        "\n",
        "pd.DataFrame({'prediction_interval': multi_result['prediction_interval'] }).to_csv(f\"./multi_result/prediction_interval.csv\")\n",
        "\n",
        "pd.DataFrame({'y_true_interval': multi_result['y_true_interval'] }).to_csv(f\"./multi_result/y_true_interval.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XTYj3ikEWBV"
      },
      "outputs": [],
      "source": [
        "#just take a single best model from pareto front\n",
        "plot_shap_vs_spend(multi_results[0][\"df_shap_values\"], multi_results[0][\"x_input_interval_nontransformed\"], multi_results[0][\"x_input_interval_transformed\"], features, media_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hpIXSgg4qJp"
      },
      "outputs": [],
      "source": [
        "#  model=final_xgboost_model,\n",
        "#     base_df_window=window_df,\n",
        "#     feature_cols=features,\n",
        "#     media_cols=media_channels,\n",
        "#     organic_cols=organic_channels,\n",
        "#     adstock_params=adstock_params_used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBj3THj34slH"
      },
      "outputs": [],
      "source": [
        "# base_df_window=window_df\n",
        "# window_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0JbQsN44J-A"
      },
      "outputs": [],
      "source": [
        "# feature_cols=features\n",
        "# media_cols=media_channels\n",
        "# organic_cols=organic_channels\n",
        "# adstock_params=adstock_params_used\n",
        "# feature_cols, media_cols, organic_cols, adstock_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzrOeBxxEXU7"
      },
      "outputs": [],
      "source": [
        "# --- imports needed for new tasks ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# --- pick adstock params consistent with your final model ---\n",
        "# If you saved them elsewhere, replace with that source\n",
        "adstock_params_used = experiment_multi.best_trials[0].user_attrs[\"adstock_alphas\"]\n",
        "\n",
        "# ---------- NEW: helper to apply adstock like training ----------\n",
        "def apply_adstock_to_df(df, channels, adstock_params):\n",
        "    df2 = df.copy()\n",
        "    for c in channels:\n",
        "        x = df2[c].values.reshape(-1, 1)\n",
        "        df2[c] = AdstockGeometric(alpha=adstock_params[c]).fit_transform(x)\n",
        "    return df2\n",
        "\n",
        "# ---------- NEW: ROAS / ROI via counterfactual contributions ----------\n",
        "def compute_contributions_roas(model, base_df_window, feature_cols, media_cols, organic_cols, adstock_params):\n",
        "    # prepare adstock-transformed features for window (same as training)\n",
        "    tdf = apply_adstock_to_df(base_df_window, media_cols + organic_cols, adstock_params)\n",
        "    X_full = tdf[feature_cols]\n",
        "    y_full = model.predict(X_full).sum()\n",
        "\n",
        "    spend = base_df_window[media_cols].sum()  # total spend per channel in window\n",
        "    contrib = {}\n",
        "\n",
        "    for c in media_cols:\n",
        "        X_zero = X_full.copy()\n",
        "        X_zero[c] = 0.0\n",
        "        y_zero = model.predict(X_zero).sum()\n",
        "        contrib[c] = y_full - y_zero\n",
        "\n",
        "    contrib_s = pd.Series(contrib, name=\"contribution\")\n",
        "    spend_s = spend.rename(\"spend\")\n",
        "    df = pd.concat([spend_s, contrib_s], axis=1)\n",
        "    df[\"roas\"] = df[\"contribution\"] / df[\"spend\"].replace(0, np.nan)\n",
        "    df[\"roi\"] = df[\"roas\"] - 1.0\n",
        "    return df\n",
        "\n",
        "# ---------- NEW: marginal ROAS around current spend path ----------\n",
        "def compute_mroas(model, base_df_window, feature_cols, media_cols, organic_cols, adstock_params, eps=0.01):\n",
        "    mroas = {}\n",
        "    base_spend = base_df_window[media_cols].sum()\n",
        "\n",
        "    for c in media_cols:\n",
        "        up = base_df_window.copy(); up[c] = up[c] * (1.0 + eps)\n",
        "        dn = base_df_window.copy(); dn[c] = dn[c] * (1.0 - eps)\n",
        "\n",
        "        up_t = apply_adstock_to_df(up, media_cols + organic_cols, adstock_params)\n",
        "        dn_t = apply_adstock_to_df(dn, media_cols + organic_cols, adstock_params)\n",
        "\n",
        "        rev_up = model.predict(up_t[feature_cols]).sum()\n",
        "        rev_dn = model.predict(dn_t[feature_cols]).sum()\n",
        "\n",
        "        denom = ( (up[c].sum() - dn[c].sum()) )\n",
        "        mroas[c] = (rev_up - rev_dn) / denom if denom != 0 else np.nan\n",
        "\n",
        "    return pd.Series(mroas, name=\"mroas\")\n",
        "\n",
        "# ---------- NEW: budget optimizer (profit) ----------\n",
        "# Drop-in replacement: same function name, same outputs; only adds total_budget param.\n",
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def optimize_budget_profit(model, base_df_window, feature_cols, media_cols, organic_cols, adstock_params,\n",
        "                           lb_mult=0.5, ub_mult=2.0, total_budget=None):\n",
        "    current_spend = base_df_window[media_cols].sum()\n",
        "    S0 = np.ones(len(media_cols))\n",
        "    lbs = np.array([lb_mult]*len(media_cols))\n",
        "    ubs = np.array([ub_mult]*len(media_cols))\n",
        "\n",
        "    def eval_revenue_and_spend(scales):\n",
        "        df = base_df_window.copy()\n",
        "        for i, c in enumerate(media_cols):\n",
        "            df[c] = df[c] * scales[i]\n",
        "        dt = apply_adstock_to_df(df, media_cols + organic_cols, adstock_params)\n",
        "        rev = model.predict(dt[feature_cols]).sum()\n",
        "        tot_spend = df[media_cols].sum().sum()\n",
        "        return rev, tot_spend\n",
        "\n",
        "    # === key line changed: allow a custom fixed budget ===\n",
        "    target_total = float(current_spend.sum() if total_budget is None else total_budget)\n",
        "\n",
        "    cons = [{\n",
        "        \"type\": \"eq\",\n",
        "        \"fun\": lambda s: sum(base_df_window[c].sum()*s[i] for i,c in enumerate(media_cols)) - target_total\n",
        "    }]\n",
        "    bnds = list(zip(lbs, ubs))\n",
        "\n",
        "    def objective(s):\n",
        "        rev, tot_spend = eval_revenue_and_spend(s)\n",
        "        return -(rev - tot_spend)  # same objective; under fixed total, this equals maximizing revenue\n",
        "\n",
        "    res = minimize(objective, x0=S0, bounds=bnds, constraints=cons, method=\"SLSQP\")\n",
        "    scales_opt = res.x\n",
        "    rev_opt, spend_opt = eval_revenue_and_spend(scales_opt)\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"channel\": media_cols,\n",
        "        \"scale\": scales_opt,\n",
        "        \"current_spend\": current_spend.values,\n",
        "        \"new_spend\": current_spend.values * scales_opt\n",
        "    })\n",
        "    out[\"delta_spend\"] = out[\"new_spend\"] - out[\"current_spend\"]\n",
        "    return out, {\"total_spend\": spend_opt, \"total_revenue\": rev_opt, \"profit\": rev_opt - spend_opt, \"success\": res.success}\n",
        "\n",
        "# ---------- NEW: budget optimizer to hit a revenue target with min spend ----------\n",
        "def optimize_budget_to_target(model, base_df_window, feature_cols, media_cols, organic_cols, adstock_params,\n",
        "                              revenue_target, lb_mult=0.0, ub_mult=3.0):\n",
        "    current_spend = base_df_window[media_cols].sum()\n",
        "    S0 = np.ones(len(media_cols))\n",
        "    lbs = np.array([lb_mult]*len(media_cols))\n",
        "    ubs = np.array([ub_mult]*len(media_cols))\n",
        "\n",
        "    def eval_revenue_and_spend(scales):\n",
        "        df = base_df_window.copy()\n",
        "        for i, c in enumerate(media_cols):\n",
        "            df[c] = df[c] * scales[i]\n",
        "        dt = apply_adstock_to_df(df, media_cols + organic_cols, adstock_params)\n",
        "        rev = model.predict(dt[feature_cols]).sum()\n",
        "        tot_spend = df[media_cols].sum().sum()\n",
        "        return rev, tot_spend\n",
        "\n",
        "    # constraint: revenue >= target\n",
        "    cons = [{\"type\":\"ineq\", \"fun\": lambda s: eval_revenue_and_spend(s)[0] - revenue_target}]\n",
        "    bnds = list(zip(lbs, ubs))\n",
        "\n",
        "    def objective(s):\n",
        "        _, tot_spend = eval_revenue_and_spend(s)\n",
        "        return tot_spend  # minimize spend\n",
        "\n",
        "    res = minimize(objective, x0=S0, bounds=bnds, constraints=cons, method=\"SLSQP\")\n",
        "    scales_opt = res.x\n",
        "    rev_opt, spend_opt = eval_revenue_and_spend(scales_opt)\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"channel\": media_cols,\n",
        "        \"scale\": scales_opt,\n",
        "        \"current_spend\": current_spend.values,\n",
        "        \"new_spend\": current_spend.values * scales_opt\n",
        "    })\n",
        "    out[\"delta_spend\"] = out[\"new_spend\"] - out[\"current_spend\"]\n",
        "    return out, {\"total_spend\": spend_opt, \"total_revenue\": rev_opt, \"profit\": rev_opt - spend_opt, \"success\": res.success}\n",
        "\n",
        "# ===================== Usage on your current run =====================\n",
        "\n",
        "# Window to evaluate at \"current spend\"\n",
        "window_df = data.loc[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX-1].copy()\n",
        "\n",
        "# 1) ROI / ROAS / mROAS\n",
        "roas_df = compute_contributions_roas(\n",
        "    model=final_xgboost_model,\n",
        "    base_df_window=window_df,\n",
        "    feature_cols=features,\n",
        "    media_cols=media_channels,\n",
        "    organic_cols=organic_channels,\n",
        "    adstock_params=adstock_params_used\n",
        ")\n",
        "mroas_s = compute_mroas(\n",
        "    model=final_xgboost_model,\n",
        "    base_df_window=window_df,\n",
        "    feature_cols=features,\n",
        "    media_cols=media_channels,\n",
        "    organic_cols=organic_channels,\n",
        "    adstock_params=adstock_params_used,\n",
        "    eps=0.01\n",
        ")\n",
        "roas_df = roas_df.join(mroas_s, how=\"left\")\n",
        "print(\"ROAS / ROI / mROAS at current spend (analysis window):\")\n",
        "print(roas_df.sort_values(\"roas\", ascending=False))\n",
        "\n",
        "# 2a) Reallocate to maximize profit at same total budget (bounds ±50%)\n",
        "alloc_profit_df, profit_summary = optimize_budget_profit(\n",
        "    model=final_xgboost_model,\n",
        "    base_df_window=window_df,\n",
        "    feature_cols=features,\n",
        "    media_cols=media_channels,\n",
        "    organic_cols=organic_channels,\n",
        "    adstock_params=adstock_params_used,\n",
        "    lb_mult=0.5, ub_mult=1.5\n",
        ")\n",
        "print(\"\\nReallocation to maximize profit (same total spend):\")\n",
        "print(alloc_profit_df)\n",
        "print(\"Summary:\", profit_summary)\n",
        "\n",
        "# 2b) Find minimal spend to reach a revenue target (example: +5% vs current)\n",
        "# compute current revenue in window\n",
        "tdf_curr = apply_adstock_to_df(window_df, media_channels + organic_channels, adstock_params_used)\n",
        "rev_current = final_xgboost_model.predict(tdf_curr[features]).sum()\n",
        "target = 1.05 * rev_current  # adjust as needed\n",
        "alloc_target_df, target_summary = optimize_budget_to_target(\n",
        "    model=final_xgboost_model,\n",
        "    base_df_window=window_df,\n",
        "    feature_cols=features,\n",
        "    media_cols=media_channels,\n",
        "    organic_cols=organic_channels,\n",
        "    adstock_params=adstock_params_used,\n",
        "    revenue_target=target,\n",
        "    lb_mult=0.0, ub_mult=2.0\n",
        ")\n",
        "print(f\"\\nMinimal spend to reach revenue target ({target:.2f}):\")\n",
        "print(alloc_target_df)\n",
        "print(\"Summary:\", target_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zxdasda"
      ],
      "metadata": {
        "id": "bwJOhl42lW8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtS-XZ-wrI05"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# NEW FUNCTION 1:\n",
        "# Diminishing returns (saturation) & optimal spend per channel\n",
        "# Reuses: apply_adstock_to_df, AdstockGeometric, final_xgboost_model\n",
        "# =========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# target = \"revenue\"\n",
        "\n",
        "def find_saturation_and_optimal(model,\n",
        "                                base_df_window,\n",
        "                                feature_cols,\n",
        "                                media_cols,\n",
        "                                organic_cols,\n",
        "                                adstock_params,\n",
        "                                margin=1.0,\n",
        "                                scale_grid=None):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      curves_dict: {channel: DataFrame with columns:\n",
        "         ['channel','scale','channel_spend','total_spend','total_revenue',\n",
        "          'delta_spend','delta_revenue','mroas_fd','profit','delta_profit']}\n",
        "      summary_df: one row per channel with:\n",
        "         ['channel','base_spend','sat_spend','sat_scale','sat_mroas',\n",
        "          'opt_spend','opt_scale','opt_delta_profit']\n",
        "    \"\"\"\n",
        "    if scale_grid is None:\n",
        "        scale_grid = np.linspace(0.0, 3.0, 31)\n",
        "\n",
        "    # Baseline (others fixed)\n",
        "    tdf_base = apply_adstock_to_df(base_df_window, media_cols + organic_cols, adstock_params)\n",
        "    rev_base = model.predict(tdf_base[feature_cols]).sum()\n",
        "    spend_base_total = base_df_window[media_cols].sum().sum()\n",
        "    profit_base = rev_base - spend_base_total\n",
        "\n",
        "    curves = {}\n",
        "    summaries = []\n",
        "\n",
        "    for channel in media_cols:\n",
        "        spend_base_channel = base_df_window[channel].sum()\n",
        "        rows = []\n",
        "\n",
        "        for s in scale_grid:\n",
        "            df = base_df_window.copy()\n",
        "            df[channel] = df[channel] * s\n",
        "\n",
        "            dt = apply_adstock_to_df(df, media_cols + organic_cols, adstock_params)\n",
        "            rev = model.predict(dt[feature_cols]).sum()\n",
        "            tot_spend = df[media_cols].sum().sum()\n",
        "            ch_spend = df[channel].sum()\n",
        "\n",
        "            rows.append({\n",
        "                \"channel\": channel,\n",
        "                \"scale\": float(s),\n",
        "                \"channel_spend\": float(ch_spend),\n",
        "                \"total_spend\": float(tot_spend),\n",
        "                \"total_revenue\": float(rev),\n",
        "                \"delta_spend\": float(ch_spend - spend_base_channel),\n",
        "                \"delta_revenue\": float(rev - rev_base)\n",
        "            })\n",
        "\n",
        "        dfc = pd.DataFrame(rows).sort_values(\"channel_spend\").reset_index(drop=True)\n",
        "\n",
        "        # Finite-difference mROAS wrt channel spend\n",
        "        mroas = [np.nan]\n",
        "        for i in range(1, len(dfc)):\n",
        "            dR = dfc.loc[i, \"total_revenue\"] - dfc.loc[i-1, \"total_revenue\"]\n",
        "            dS = dfc.loc[i, \"channel_spend\"]  - dfc.loc[i-1, \"channel_spend\"]\n",
        "            mroas.append(dR / dS if dS != 0 else np.nan)\n",
        "        dfc[\"mroas_fd\"] = mroas\n",
        "\n",
        "        # Profit (unit gross margin; change 'margin' arg if needed)\n",
        "        dfc[\"profit\"] = margin*dfc[\"total_revenue\"] - dfc[\"total_spend\"]\n",
        "        dfc[\"delta_profit\"] = dfc[\"profit\"] - (margin*rev_base - spend_base_total)\n",
        "\n",
        "        # Saturation: first point where margin * mROAS < 1\n",
        "        sat_idx = None\n",
        "        for i in range(len(dfc)):\n",
        "            mr = dfc.loc[i, \"mroas_fd\"]\n",
        "            if pd.notnull(mr) and margin * mr < 1.0:\n",
        "                sat_idx = i\n",
        "                break\n",
        "\n",
        "        # Optimal (univariate): argmax delta_profit (others fixed)\n",
        "        opt_idx = int(dfc[\"delta_profit\"].idxmax())\n",
        "\n",
        "        curves[channel] = dfc\n",
        "\n",
        "        summaries.append({\n",
        "            \"channel\": channel,\n",
        "            \"base_spend\": float(dfc.loc[(dfc[\"scale\"]-1.0).abs().idxmin(), \"channel_spend\"]),\n",
        "            \"sat_spend\": float(dfc.loc[sat_idx, \"channel_spend\"]) if sat_idx is not None else np.nan,\n",
        "            \"sat_scale\": float(dfc.loc[sat_idx, \"scale\"]) if sat_idx is not None else np.nan,\n",
        "            \"sat_mroas\": float(dfc.loc[sat_idx, \"mroas_fd\"]) if sat_idx is not None else np.nan,\n",
        "            \"opt_spend\": float(dfc.loc[opt_idx, \"channel_spend\"]),\n",
        "            \"opt_scale\": float(dfc.loc[opt_idx, \"scale\"]),\n",
        "            \"opt_delta_profit\": float(dfc.loc[opt_idx, \"delta_profit\"])\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summaries)\n",
        "    return curves, summary_df\n",
        "\n",
        "\n",
        "# =========================\n",
        "# NEW FUNCTION 2:\n",
        "# Uncertainty around curves and saturation/optimal using your best Optuna trials\n",
        "# Reuses: AdstockGeometric, experiment_multi, XGBRegressor\n",
        "# =========================\n",
        "def quantify_curve_uncertainty(window_df,\n",
        "                               features,\n",
        "                               media_cols,\n",
        "                               organic_cols,\n",
        "                               data,\n",
        "                               target,\n",
        "                               experiment_multi,\n",
        "                               END_ANALYSIS_INDEX,\n",
        "                               final_model=None,\n",
        "                               adstock_params_for_final=None,\n",
        "                               top_k=5,\n",
        "                               margin=1.0,\n",
        "                               scale_grid=None):\n",
        "    \"\"\"\n",
        "    Builds a small ensemble from your Pareto/best trials + final_model (if provided),\n",
        "    recomputes per-channel curves, and returns mean / 5th / 95th percentiles.\n",
        "\n",
        "    Returns:\n",
        "      curves_ci: dict[channel] -> DataFrame with:\n",
        "         ['scale','channel_spend','rev_mean','rev_p05','rev_p95',\n",
        "          'mroas_mean','mroas_p05','mroas_p95','dprof_mean','dprof_p05','dprof_p95']\n",
        "      summary_ci: DataFrame per channel with:\n",
        "         ['channel','sat_spend_mean','sat_spend_p05','sat_spend_p95',\n",
        "          'opt_spend_mean','opt_spend_p05','opt_spend_p95']\n",
        "    \"\"\"\n",
        "    from xgboost import XGBRegressor\n",
        "\n",
        "    if scale_grid is None:\n",
        "        scale_grid = np.linspace(0.0, 3.0, 31)\n",
        "\n",
        "    # 1) Collect models: final + top_k Pareto/best trials\n",
        "    models_info = []\n",
        "    if (final_model is not None) and (adstock_params_for_final is not None):\n",
        "        models_info.append((final_model, adstock_params_for_final))\n",
        "\n",
        "    trials = experiment_multi.best_trials[:min(top_k, len(experiment_multi.best_trials))]\n",
        "    for tr in trials:\n",
        "        params = tr.user_attrs[\"params\"]\n",
        "        ads = tr.user_attrs[\"adstock_alphas\"]\n",
        "\n",
        "        # train like your model_refit (inline, no extra function)\n",
        "        data_refit = data.copy()\n",
        "        for f in media_cols + organic_cols:\n",
        "            x_feature = data_refit[f].values.reshape(-1, 1)\n",
        "            data_refit[f] = AdstockGeometric(alpha=ads[f]).fit_transform(x_feature)\n",
        "\n",
        "        X_train = data_refit.loc[0:END_ANALYSIS_INDEX-1, features]\n",
        "        y_train = data[target].values[0:END_ANALYSIS_INDEX]\n",
        "\n",
        "        model_t = XGBRegressor(random_state=0, **params)\n",
        "        model_t.fit(X_train, y_train)\n",
        "        models_info.append((model_t, ads))\n",
        "\n",
        "    # 2) For each channel, compute curves for each model and aggregate\n",
        "    curves_ci = {}\n",
        "    summary_rows = []\n",
        "\n",
        "    for ch in media_cols:\n",
        "        per_model = []\n",
        "\n",
        "        for (m, ads) in models_info:\n",
        "            # build curve (inline, same logic as function 1)\n",
        "            # baseline for profit and deltas\n",
        "            tdf_base = apply_adstock_to_df(window_df, media_cols + organic_cols, ads)\n",
        "            rev_base = m.predict(tdf_base[features]).sum()\n",
        "            spend_base_total = window_df[media_cols].sum().sum()\n",
        "            spend_base_channel = window_df[ch].sum()\n",
        "\n",
        "            rows = []\n",
        "            for s in scale_grid:\n",
        "                df = window_df.copy()\n",
        "                df[ch] = df[ch] * s\n",
        "\n",
        "                dt = apply_adstock_to_df(df, media_cols + organic_cols, ads)\n",
        "                rev = m.predict(dt[features]).sum()\n",
        "                tot_spend = df[media_cols].sum().sum()\n",
        "                ch_spend = df[ch].sum()\n",
        "\n",
        "                rows.append({\n",
        "                    \"scale\": float(s),\n",
        "                    \"channel_spend\": float(ch_spend),\n",
        "                    \"total_revenue\": float(rev),\n",
        "                    \"total_spend\": float(tot_spend),\n",
        "                    \"delta_profit\": float(margin*rev - tot_spend - (margin*rev_base - spend_base_total))\n",
        "                })\n",
        "\n",
        "            dfc = pd.DataFrame(rows).sort_values(\"channel_spend\").reset_index(drop=True)\n",
        "\n",
        "            # finite-difference mROAS\n",
        "            mroas = [np.nan]\n",
        "            for i in range(1, len(dfc)):\n",
        "                dR = dfc.loc[i, \"total_revenue\"] - dfc.loc[i-1, \"total_revenue\"]\n",
        "                dS = dfc.loc[i, \"channel_spend\"]  - dfc.loc[i-1, \"channel_spend\"]\n",
        "                mroas.append(dR / dS if dS != 0 else np.nan)\n",
        "            dfc[\"mroas_fd\"] = mroas\n",
        "            per_model.append(dfc[[\"scale\",\"channel_spend\",\"total_revenue\",\"mroas_fd\",\"delta_profit\"]])\n",
        "\n",
        "        # stack and aggregate\n",
        "        stk = []\n",
        "        for i, dfc in enumerate(per_model):\n",
        "            tmp = dfc.copy()\n",
        "            tmp[\"model_id\"] = i\n",
        "            stk.append(tmp)\n",
        "        stk = pd.concat(stk, axis=0, ignore_index=True)\n",
        "\n",
        "        agg = stk.groupby([\"scale\",\"channel_spend\"]).agg(\n",
        "            rev_mean=(\"total_revenue\",\"mean\"),\n",
        "            rev_p05 =(\"total_revenue\",lambda x: np.percentile(x,5)),\n",
        "            rev_p95 =(\"total_revenue\",lambda x: np.percentile(x,95)),\n",
        "            mroas_mean=(\"mroas_fd\",\"mean\"),\n",
        "            mroas_p05 =(\"mroas_fd\",lambda x: np.percentile(x.dropna(),5) if x.notna().any() else np.nan),\n",
        "            mroas_p95 =(\"mroas_fd\",lambda x: np.percentile(x.dropna(),95) if x.notna().any() else np.nan),\n",
        "            dprof_mean=(\"delta_profit\",\"mean\"),\n",
        "            dprof_p05 =(\"delta_profit\",lambda x: np.percentile(x,5)),\n",
        "            dprof_p95 =(\"delta_profit\",lambda x: np.percentile(x,95))\n",
        "        ).reset_index()\n",
        "\n",
        "        curves_ci[ch] = agg\n",
        "\n",
        "        # summarize saturation & optimal across models\n",
        "        sat_list = []\n",
        "        opt_list = []\n",
        "        # rebuild per-model to compute sat/opt per model cleanly\n",
        "        n_models = stk[\"model_id\"].nunique()\n",
        "        for mid in range(n_models):\n",
        "            dfm = stk[stk[\"model_id\"] == mid].sort_values(\"channel_spend\").reset_index(drop=True)\n",
        "\n",
        "            # saturation: first where margin*mROAS < 1\n",
        "            sat_idx = None\n",
        "            for i in range(len(dfm)):\n",
        "                mr = dfm.loc[i, \"mroas_fd\"]\n",
        "                if pd.notnull(mr) and margin * mr < 1.0:\n",
        "                    sat_idx = i\n",
        "                    break\n",
        "            sat_list.append(float(dfm.loc[sat_idx, \"channel_spend\"]) if sat_idx is not None else np.nan)\n",
        "\n",
        "            # optimal: argmax delta_profit\n",
        "            opt_idx = int(dfm[\"delta_profit\"].idxmax())\n",
        "            opt_list.append(float(dfm.loc[opt_idx, \"channel_spend\"]))\n",
        "\n",
        "        # percentiles ignoring NaNs for saturation\n",
        "        sat_vals = [v for v in sat_list if pd.notnull(v)]\n",
        "        sat_mean = np.nan if len(sat_vals)==0 else float(np.mean(sat_vals))\n",
        "        sat_p05 = np.nan if len(sat_vals)==0 else float(np.percentile(sat_vals,5))\n",
        "        sat_p95 = np.nan if len(sat_vals)==0 else float(np.percentile(sat_vals,95))\n",
        "\n",
        "        summary_rows.append({\n",
        "            \"channel\": ch,\n",
        "            \"sat_spend_mean\": sat_mean,\n",
        "            \"sat_spend_p05\": sat_p05,\n",
        "            \"sat_spend_p95\": sat_p95,\n",
        "            \"opt_spend_mean\": float(np.mean(opt_list)),\n",
        "            \"opt_spend_p05\": float(np.percentile(opt_list,5)),\n",
        "            \"opt_spend_p95\": float(np.percentile(opt_list,95))\n",
        "        })\n",
        "\n",
        "    summary_ci = pd.DataFrame(summary_rows)\n",
        "    return curves_ci, summary_ci\n",
        "\n",
        "\n",
        "# Window\n",
        "window_df = data.loc[START_ANALYSIS_INDEX:END_ANALYSIS_INDEX-1].copy()\n",
        "\n",
        "# 1) Diminishing returns & optimal spend (others fixed at baseline)\n",
        "curves, summary = find_saturation_and_optimal(\n",
        "    model=final_xgboost_model,\n",
        "    base_df_window=window_df,\n",
        "    feature_cols=features,\n",
        "    media_cols=media_channels,\n",
        "    organic_cols=organic_channels,\n",
        "    adstock_params=adstock_params_used,  # from your best trial\n",
        "    margin=1.0,                          # set to your gross margin, e.g., 0.6 if needed\n",
        "    scale_grid=np.linspace(0.0, 3.0, 31)\n",
        ")\n",
        "print(\"=== Per-channel saturation & optimal spend ===\")\n",
        "print(summary.sort_values(\"opt_delta_profit\", ascending=False))\n",
        "# Example: inspect a curve\n",
        "# curves['search_S'].head()\n",
        "\n",
        "# 2) Uncertainty using your best Optuna trials (+ final model)\n",
        "curves_ci, summary_ci = quantify_curve_uncertainty(\n",
        "    window_df=window_df,\n",
        "    features=features,\n",
        "    media_cols=media_channels,\n",
        "    organic_cols=organic_channels,\n",
        "    data=data,\n",
        "    target=\"revenue\",\n",
        "    experiment_multi=experiment_multi,\n",
        "    END_ANALYSIS_INDEX=END_ANALYSIS_INDEX,\n",
        "    final_model=final_xgboost_model,\n",
        "    adstock_params_for_final=adstock_params_used,\n",
        "    top_k=5,\n",
        "    margin=1.0,\n",
        "    scale_grid=np.linspace(0.0, 3.0, 31)\n",
        ")\n",
        "print(\"\\n=== Uncertainty (mean / 5th / 95th percentiles) ===\")\n",
        "print(summary_ci)\n",
        "# Example: inspect CI curve for a channel\n",
        "# curves_ci['search_S'].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NKfjvrrrIyh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5FqaixPrIwh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qa3KRSBrIt4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTZxjjA_8BpI"
      },
      "outputs": [],
      "source": [
        "# model=final_xgboost_model\n",
        "# base_df_window=window_df.copy()\n",
        "# feature_cols=features\n",
        "# media_cols=media_channels\n",
        "# organic_cols=organic_channels\n",
        "# adstock_params=adstock_params_used\n",
        "\n",
        "\n",
        "\n",
        "# tdf = apply_adstock_to_df(base_df_window, media_cols + organic_cols, adstock_params)\n",
        "# X_full = tdf[feature_cols]\n",
        "# y_full = model.predict(X_full).sum()\n",
        "\n",
        "# spend = base_df_window[media_cols].sum()  # total spend per channel in window\n",
        "# contrib = {}\n",
        "\n",
        "# for c in media_cols:\n",
        "#     X_zero = X_full.copy()\n",
        "#     X_zero[c] = 0.0\n",
        "#     y_zero = model.predict(X_zero).sum()\n",
        "#     contrib[c] = y_full - y_zero\n",
        "\n",
        "# contrib_s = pd.Series(contrib, name=\"contribution\")\n",
        "# spend_s = spend.rename(\"spend\")\n",
        "# df = pd.concat([spend_s, contrib_s], axis=1)\n",
        "# df[\"roas\"] = df[\"contribution\"] / df[\"spend\"].replace(0, np.nan)\n",
        "# df[\"roi\"] = df[\"roas\"] - 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KliTMMdn8BnC"
      },
      "outputs": [],
      "source": [
        "spend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyGyxCA38Bk9"
      },
      "outputs": [],
      "source": [
        "contrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8htNx8E8Bip"
      },
      "outputs": [],
      "source": [
        "spend_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BPBGAuS8yA9"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjn5TFCj8x-4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHR-mq6N8x8j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmfll9sa3NpG"
      },
      "outputs": [],
      "source": [
        "roas_df.columns, alloc_profit_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1srhxAm2BCM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Ensure 'channel' exists in roas_df (use index if needed) ---\n",
        "if \"channel\" not in roas_df.columns:\n",
        "    roas_df = roas_df.copy()\n",
        "    roas_df[\"channel\"] = roas_df.index\n",
        "\n",
        "# --- ROAS vs mROAS ---\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "roas_df.set_index(\"channel\")[[\"roas\",\"mroas\"]].plot(kind=\"bar\", ax=ax)\n",
        "ax.axhline(1.0, linestyle=\"--\", label=\"Break-even ROAS=1\")\n",
        "ax.set_ylabel(\"ROAS / mROAS\")\n",
        "ax.set_title(\"Average vs Marginal ROAS by Channel\")\n",
        "ax.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Spend Reallocation: current vs new ---\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "alloc_profit_df.set_index(\"channel\")[[\"current_spend\",\"new_spend\"]].plot(kind=\"bar\", ax=ax)\n",
        "ax.set_ylabel(\"Spend\")\n",
        "ax.set_title(\"Spend Reallocation (Profit Maximization)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Optional: Delta spend (clear view of budget moves) ---\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "alloc_profit_df.set_index(\"channel\")[[\"delta_spend\"]].plot(kind=\"bar\", ax=ax)\n",
        "ax.set_ylabel(\"Δ Spend\")\n",
        "ax.set_title(\"Change in Spend by Channel\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lHCCvzv3MA5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}